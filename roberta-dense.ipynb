{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cafe2470-15f3-4927-b7c0-3d97dd0b8dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/deb/.local/lib/python3.10/site-packages (4.40.0)\n",
      "Requirement already satisfied: filelock in /home/tools/anaconda3/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/deb/.local/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/deb/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tools/anaconda3/lib/python3.10/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tools/anaconda3/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tools/anaconda3/lib/python3.10/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /home/tools/anaconda3/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/deb/.local/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/deb/.local/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/tools/anaconda3/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/deb/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/deb/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tools/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tools/anaconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tools/anaconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tools/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytorch-ignite in /home/deb/.local/lib/python3.10/site-packages (0.5.0.post2)\n",
      "Requirement already satisfied: torch<3,>=1.3 in /home/deb/.local/lib/python3.10/site-packages (from pytorch-ignite) (2.2.2)\n",
      "Requirement already satisfied: packaging in /home/tools/anaconda3/lib/python3.10/site-packages (from pytorch-ignite) (22.0)\n",
      "Requirement already satisfied: filelock in /home/tools/anaconda3/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/tools/anaconda3/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/tools/anaconda3/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /home/tools/anaconda3/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/deb/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=1.3->pytorch-ignite) (12.4.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tools/anaconda3/lib/python3.10/site-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/tools/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch<3,>=1.3->pytorch-ignite) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip  install transformers\n",
    "!pip install pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002fcde5-17a0-478f-8d95-570f4e655c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "from transformers import BertTokenizer,BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.engine.engine import Engine, State, Events\n",
    "from ignite.handlers import EarlyStopping\n",
    "from ignite.contrib.handlers import TensorboardLogger, ProgressBar\n",
    "from ignite.utils import convert_tensor\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b789f2-268c-43b8-a19b-853fab785e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc22ea3-00bc-4a62-b0b4-cd090a04e131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.8/site-packages (0.1.97)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e17946b2-753c-4d30-b632-51883f1594aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a53ffa65-a76f-4851-ad9f-98a3716a845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.En.csv')\n",
    "df=df[['tweet','sarcastic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5c8e0e-deff-4bc2-8972-f0def0562f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = \\\n",
    "              np.split(df.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(df)), int(.8*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1601dec-6b3d-4439-999f-42cef649a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.concat([train, validate], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbd2a338-c9a6-4d90-9a4c-083bfef0726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer , TrainingArguments\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb1d3c5e-d40c-4fcb-a4a9-497613d0ff6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.19.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.22.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from datasets) (3.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (2.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72cde166-2d70-4830-9f2c-2275f8bb89a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCLTrainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = df['tweet'].values\n",
    "        self.label = df['sarcastic'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        inputs_text = self.tokenizer.encode_plus(\n",
    "                                text,\n",
    "                                truncation=True,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length'\n",
    "                            )         \n",
    "        target = self.label[index]\n",
    "        \n",
    "        text_ids = inputs_text['input_ids']\n",
    "        text_mask = inputs_text['attention_mask']\n",
    "        \n",
    "        return {\n",
    "            'text_ids': torch.tensor(text_ids, dtype=torch.long),\n",
    "            'text_mask': torch.tensor(text_mask, dtype=torch.long),\n",
    "            'target': torch.tensor(target, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dccc3f15-b00f-45f8-9550-587901992e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class F1_Loss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2).to(torch.float32)\n",
    "        \n",
    "        tp = (y_true_one_hot * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true_one_hot) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true_one_hot) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true_one_hot * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\n",
    "        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n",
    "\n",
    "        CE = torch.nn.CrossEntropyLoss(weight=(1 - f1))(y_pred, y_true_one_hot)\n",
    "        return CE.mean()\n",
    "    \n",
    "def criterion(outputs1,  targets):\n",
    "    criterion = F1_Loss()\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(outputs1, targets)\n",
    "    return loss\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1dc6e931-8f79-462e-880f-4e24fc052404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PCL_Model_Arch(nn.Module):\n",
    "    def __init__(self, pre_trained='roberta-base', hidden_dim=768, num_dense_layers=2, dense_dim=256):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pre_trained)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.LSTM = nn.LSTM(self.hidden_size, hidden_dim, bidirectional=True)\n",
    "        \n",
    "        # Define the dense layers\n",
    "        dense_layers = []\n",
    "        dense_layers.append(nn.Linear(hidden_dim * 2, dense_dim))  # Adjust the input size\n",
    "        dense_layers.append(nn.ReLU())\n",
    "        dense_layers.append(nn.Dropout(0.2))\n",
    "        for _ in range(num_dense_layers - 1):  # Remaining dense layers\n",
    "            dense_layers.append(nn.Linear(dense_dim, dense_dim))\n",
    "            dense_layers.append(nn.ReLU())\n",
    "            dense_layers.append(nn.Dropout(0.2))\n",
    "        self.dense_layers = nn.Sequential(*dense_layers)\n",
    "\n",
    "        self.clf = nn.Linear(dense_dim, 2)\n",
    "\n",
    "    def forward(self, id, mask):\n",
    "        encoded_layers = self.bert(input_ids=id, attention_mask=mask)\n",
    "        encoded_layers = encoded_layers[0].permute(1, 0, 2)\n",
    "        enc_hiddens, (last_hidden, last_cell) = self.LSTM(encoded_layers)\n",
    "        output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)\n",
    "\n",
    "        # Pass the LSTM output through the dense layers\n",
    "        output_hidden = self.dense_layers(output_hidden)\n",
    "\n",
    "        output = self.clf(output_hidden)\n",
    "        return F.softmax(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59cc3aef-940f-42bc-a7c9-ddde0205a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7669a0b-5019-4f97-b6cb-c16b7b01b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        \n",
    "        text_ids = data['text_ids'].to(device, dtype=torch.long)\n",
    "        text_mask = data['text_mask'].to(device, dtype=torch.long)\n",
    "        targets = data['target'].to(device, dtype=torch.long)       \n",
    "        batch_size = text_ids.size(0)\n",
    "        outputs = model(text_ids, text_mask)       \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss = loss / 1\n",
    "        loss.backward()\n",
    "        \n",
    "    \n",
    "        if (step + 1) % 1 == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbc8d52e-75d6-4458-9f5f-e4fb4b20167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:        \n",
    "        \n",
    "        text_ids = data['text_ids'].to(device, dtype=torch.long)\n",
    "        text_mask = data['text_mask'].to(device, dtype=torch.long)\n",
    "        targets = data['target'].to(device, dtype=torch.long)       \n",
    "        batch_size = text_ids.size(0)\n",
    "        outputs = model(text_ids, text_mask)       \n",
    "        loss = criterion(outputs, targets)\n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])   \n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f47bc1f5-703b-4e7e-b0f9-484ee7c9f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, optimizer, scheduler, device, num_epochs):\n",
    "    # To automatically log gradients   \n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_loss = np.inf\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=device, epoch=epoch)\n",
    "        \n",
    "        val_epoch_loss = valid_one_epoch(model, valid_loader, device=device, \n",
    "                                         epoch=epoch)\n",
    "    \n",
    "        history['Train Loss'].append(train_epoch_loss)\n",
    "        history['Valid Loss'].append(val_epoch_loss)\n",
    "        \n",
    "       \n",
    "        \n",
    "        # deep copy the model\n",
    "        if val_epoch_loss <= best_epoch_loss:\n",
    "            print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n",
    "            best_epoch_loss = val_epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = f\"bert_tweet/roberta-dense-attn.bin\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            print(\"Model Saved\")\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c425c448-9ded-44dc-8d8d-38053201122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders(train_df, valid_df):\n",
    "    train_dataset = PCLTrainDataset(train_df, tokenizer=tokenizer, max_length=120)\n",
    "    valid_dataset = PCLTrainDataset(valid_df, tokenizer=tokenizer, max_length=120)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, \n",
    "                              num_workers=2, shuffle=False, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=16, \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9aa1aed7-898f-4e4f-a53b-a9b5a10b72a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1268"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef83eecf-c2d5-4563-ba24-8fb7b0c5db67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|█████████████████████████████████████| 156/156 [06:49<00:00,  2.63s/it, Epoch=1, LR=3.91e-5, Train_Loss=0.273]\n",
      "100%|███████████████████████████████████████| 18/18 [00:08<00:00,  2.20it/s, Epoch=1, LR=3.91e-5, Valid_Loss=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (inf ---> 0.24509894504821558)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 156/156 [06:32<00:00,  2.51s/it, Epoch=2, LR=1.62e-5, Train_Loss=0.271]\n",
      "100%|███████████████████████████████████████| 18/18 [00:08<00:00,  2.13it/s, Epoch=2, LR=1.62e-5, Valid_Loss=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (0.24509894504821558 ---> 0.2450042325172493)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 156/156 [06:24<00:00,  2.46s/it, Epoch=3, LR=1.49e-6, Train_Loss=0.271]\n",
      "100%|███████████████████████████████████████| 18/18 [00:07<00:00,  2.27it/s, Epoch=3, LR=1.49e-6, Valid_Loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (0.2450042325172493 ---> 0.2423704180786078)\n",
      "Model Saved\n",
      "\n",
      "Training complete in 0h 20m 18s\n",
      "Best Loss: 0.2424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, valid_df = train_test_split(train, test_size=0.1, random_state=42)\n",
    "\n",
    "train_loader, valid_loader = prepare_loaders(train_df, valid_df)\n",
    "\n",
    "# model = PCL_Model_Arch()\n",
    "model = PCL_Model_Arch(num_dense_layers=2, dense_dim=256)\n",
    "model.to(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=1e-6)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=500, eta_min=1e-6)\n",
    "\n",
    "model, history = run_training(model, optimizer, scheduler,\n",
    "                              device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                              num_epochs=3)\n",
    "\n",
    "del model, history, train_loader, valid_loader\n",
    "_ = gc.collect()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "258aa4a7-3a69-4d1f-8d6c-736badbbf345",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28b2fafd-4ac9-4f4a-a658-eeb0d30c6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = PCLTrainDataset(test, tokenizer=tokenizer, max_length=120)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, \n",
    "                         num_workers=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d85aa36e-8891-4f2f-9333-1713d2dc58fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    PREDS = []\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        ids = data['text_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['text_mask'].to(device, dtype = torch.long)\n",
    "        outputs = model(ids, mask)\n",
    "        PREDS.append(outputs.detach().cpu().numpy()) \n",
    "    \n",
    "    PREDS = np.concatenate(PREDS)\n",
    "    gc.collect()\n",
    "    \n",
    "    return PREDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6b2af038-8f29-4808-80b7-4b40021226c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model_paths, dataloader, device):\n",
    "    final_preds = []\n",
    "    for i, path in enumerate(model_paths):\n",
    "        model = PCL_Model_Arch()\n",
    "        model.to(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        \n",
    "        print(f\"Getting predictions for model {i+1}\")\n",
    "        preds = valid_fn(model, dataloader, device)\n",
    "        final_preds.append(preds)\n",
    "    \n",
    "    final_preds = np.array(final_preds)\n",
    "    final_preds = np.mean(final_preds, axis=0)\n",
    "    final_preds= np.argmax(final_preds,axis=1)\n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ea23c48-8c3d-42e3-8dd8-e772cde99aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 44/44 [00:19<00:00,  2.23it/s]\n"
     ]
    }
   ],
   "source": [
    "#MODEL_PATH_2=['bert_tweet/Loss-Fold-0.bin','bert_tweet/Loss-Fold-1.bin','bert_tweet/Loss-Fold-2.bin','bert_tweet/Loss-Fold-3.bin','bert_tweet/Loss-Fold-4.bin']\n",
    "MODEL_PATH_2=['bert_tweet/roberta-dense-attn.bin']\n",
    "preds = inference(MODEL_PATH_2, valid_loader, torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd75680e-f31c-420c-ab8b-71c8b2eb7dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score,f1_score,accuracy_score,recall_score,precision_score,classification_report\n",
    "def print_statistics(y, y_pred):\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision =precision_score(y, y_pred, average='weighted')\n",
    "    recall = recall_score(y, y_pred, average='weighted')\n",
    "    f_score = f1_score(y, y_pred, average='weighted')\n",
    "    print('Accuracy: %.3f\\nPrecision: %.3f\\nRecall: %.3f\\nF_score: %.3f\\n'\n",
    "          % (accuracy, precision, recall, f_score))\n",
    "    print(classification_report(y, y_pred))\n",
    "    return accuracy, precision, recall, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "adcf6130-b220-499e-95c6-6587417193f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.766\n",
      "Precision: 0.587\n",
      "Recall: 0.766\n",
      "F_score: 0.665\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87       531\n",
      "           1       0.00      0.00      0.00       162\n",
      "\n",
      "    accuracy                           0.77       693\n",
      "   macro avg       0.38      0.50      0.43       693\n",
      "weighted avg       0.59      0.77      0.66       693\n",
      "\n",
      "(0.7662337662337663, 0.587114184516782, 0.7662337662337663, 0.664820473644003)\n"
     ]
    }
   ],
   "source": [
    "print(print_statistics(test['sarcastic'],preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906070dc-6b4c-49e4-bff6-ab157d7574d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
