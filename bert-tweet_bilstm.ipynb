{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cafe2470-15f3-4927-b7c0-3d97dd0b8dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/deb/.local/lib/python3.10/site-packages (4.40.0)\n",
      "Requirement already satisfied: filelock in /home/tools/anaconda3/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/deb/.local/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/deb/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tools/anaconda3/lib/python3.10/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tools/anaconda3/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tools/anaconda3/lib/python3.10/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /home/tools/anaconda3/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/deb/.local/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/deb/.local/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/tools/anaconda3/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/deb/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/deb/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tools/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tools/anaconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tools/anaconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tools/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytorch-ignite in /home/deb/.local/lib/python3.10/site-packages (0.5.0.post2)\n",
      "Requirement already satisfied: torch<3,>=1.3 in /home/deb/.local/lib/python3.10/site-packages (from pytorch-ignite) (2.2.2)\n",
      "Requirement already satisfied: packaging in /home/tools/anaconda3/lib/python3.10/site-packages (from pytorch-ignite) (22.0)\n",
      "Requirement already satisfied: filelock in /home/tools/anaconda3/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/tools/anaconda3/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/tools/anaconda3/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /home/tools/anaconda3/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/deb/.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/deb/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=1.3->pytorch-ignite) (12.4.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tools/anaconda3/lib/python3.10/site-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/tools/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch<3,>=1.3->pytorch-ignite) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip  install transformers\n",
    "!pip install pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002fcde5-17a0-478f-8d95-570f4e655c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "from transformers import BertTokenizer,BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.engine.engine import Engine, State, Events\n",
    "from ignite.handlers import EarlyStopping\n",
    "from ignite.contrib.handlers import TensorboardLogger, ProgressBar\n",
    "from ignite.utils import convert_tensor\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b789f2-268c-43b8-a19b-853fab785e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc22ea3-00bc-4a62-b0b4-cd090a04e131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/deb/.local/lib/python3.10/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e17946b2-753c-4d30-b632-51883f1594aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a53ffa65-a76f-4851-ad9f-98a3716a845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.En.csv')\n",
    "df=df[['tweet','sarcastic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5c8e0e-deff-4bc2-8972-f0def0562f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = \\\n",
    "              np.split(df.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(df)), int(.8*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1601dec-6b3d-4439-999f-42cef649a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.concat([train, validate], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbd2a338-c9a6-4d90-9a4c-083bfef0726b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 15:29:03.100999: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-22 15:29:03.215316: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer , TrainingArguments\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb1d3c5e-d40c-4fcb-a4a9-497613d0ff6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/deb/.local/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in /home/tools/anaconda3/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/deb/.local/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/deb/.local/lib/python3.10/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/deb/.local/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/deb/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/tools/anaconda3/lib/python3.10/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/tools/anaconda3/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/tools/anaconda3/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /home/deb/.local/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/deb/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /home/deb/.local/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/deb/.local/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/deb/.local/lib/python3.10/site-packages (from datasets) (0.22.1)\n",
      "Requirement already satisfied: packaging in /home/tools/anaconda3/lib/python3.10/site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tools/anaconda3/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/deb/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/tools/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/deb/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/deb/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/deb/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/deb/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/deb/.local/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tools/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tools/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tools/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tools/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/tools/anaconda3/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/tools/anaconda3/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/tools/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72cde166-2d70-4830-9f2c-2275f8bb89a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCLTrainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = df['tweet'].values\n",
    "        self.label = df['sarcastic'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        inputs_text = self.tokenizer.encode_plus(\n",
    "                                text,\n",
    "                                truncation=True,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length'\n",
    "                            )         \n",
    "        target = self.label[index]\n",
    "        \n",
    "        text_ids = inputs_text['input_ids']\n",
    "        text_mask = inputs_text['attention_mask']\n",
    "        \n",
    "        return {\n",
    "            'text_ids': torch.tensor(text_ids, dtype=torch.long),\n",
    "            'text_mask': torch.tensor(text_mask, dtype=torch.long),\n",
    "            'target': torch.tensor(target, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dccc3f15-b00f-45f8-9550-587901992e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class F1_Loss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_true_one_hot = F.one_hot(y_true.to(torch.int64), 2).to(torch.float32)\n",
    "        \n",
    "        tp = (y_true_one_hot * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true_one_hot) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true_one_hot) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true_one_hot * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\n",
    "        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n",
    "\n",
    "        CE = torch.nn.CrossEntropyLoss(weight=(1 - f1))(y_pred, y_true_one_hot)\n",
    "        return CE.mean()\n",
    "    \n",
    "def criterion(outputs1,  targets):\n",
    "    criterion = F1_Loss()\n",
    "    loss = criterion(outputs1, targets)\n",
    "    return loss\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dc6e931-8f79-462e-880f-4e24fc052404",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCL_Model_Arch(nn.Module):\n",
    "    def __init__(self,pre_trained='vinai/bertweet-base'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = AutoModel.from_pretrained(pre_trained)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.LSTM = nn.LSTM(self.hidden_size,self.hidden_size,bidirectional=True)\n",
    "        self.clf = nn.Linear(self.hidden_size*2,2)\n",
    "        \n",
    "    def forward(self,id,mask):\n",
    "        \n",
    "        encoded_layers = self.bert(input_ids=id,attention_mask=mask)\n",
    "        encoded_layers = encoded_layers[0].permute(1, 0, 2)\n",
    "        enc_hiddens, (last_hidden, last_cell) = self.LSTM(encoded_layers)\n",
    "        output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)\n",
    "        output_hidden = F.dropout(output_hidden,0.2)\n",
    "        output = self.clf(output_hidden)\n",
    "        \n",
    "        return F.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59cc3aef-940f-42bc-a7c9-ddde0205a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained('vinai/bertweet-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7669a0b-5019-4f97-b6cb-c16b7b01b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        \n",
    "        text_ids = data['text_ids'].to(device, dtype=torch.long)\n",
    "        text_mask = data['text_mask'].to(device, dtype=torch.long)\n",
    "        targets = data['target'].to(device, dtype=torch.long)       \n",
    "        batch_size = text_ids.size(0)\n",
    "        outputs = model(text_ids, text_mask)       \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss = loss / 1\n",
    "        loss.backward()\n",
    "    \n",
    "        if (step + 1) % 1 == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbc8d52e-75d6-4458-9f5f-e4fb4b20167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:        \n",
    "        \n",
    "        text_ids = data['text_ids'].to(device, dtype=torch.long)\n",
    "        text_mask = data['text_mask'].to(device, dtype=torch.long)\n",
    "        targets = data['target'].to(device, dtype=torch.long)       \n",
    "        batch_size = text_ids.size(0)\n",
    "        outputs = model(text_ids, text_mask)       \n",
    "        loss = criterion(outputs, targets)\n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])   \n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f47bc1f5-703b-4e7e-b0f9-484ee7c9f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, optimizer, scheduler, device, num_epochs):\n",
    "    # To automatically log gradients   \n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_loss = np.inf\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=device, epoch=epoch)\n",
    "        \n",
    "        val_epoch_loss = valid_one_epoch(model, valid_loader, device=device, \n",
    "                                         epoch=epoch)\n",
    "    \n",
    "        history['Train Loss'].append(train_epoch_loss)\n",
    "        history['Valid Loss'].append(val_epoch_loss)\n",
    "        \n",
    "       \n",
    "        \n",
    "        # deep copy the model\n",
    "        if val_epoch_loss <= best_epoch_loss:\n",
    "            print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n",
    "            best_epoch_loss = val_epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = f\"bert_tweet/Loss-Fold-0.bin\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            print(\"Model Saved\")\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c425c448-9ded-44dc-8d8d-38053201122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders(train_df, valid_df):\n",
    "    train_dataset = PCLTrainDataset(train_df, tokenizer=tokenizer, max_length=120)\n",
    "    valid_dataset = PCLTrainDataset(valid_df, tokenizer=tokenizer, max_length=120)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, \n",
    "                              num_workers=2, shuffle=False, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=16, \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9aa1aed7-898f-4e4f-a53b-a9b5a10b72a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5807"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef83eecf-c2d5-4563-ba24-8fb7b0c5db67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 156/156 [00:13<00:00, 11.24it/s, Epoch=1, LR=7.81e-5, Train_Loss=0.273]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:00<00:00, 29.06it/s, Epoch=1, LR=7.81e-5, Valid_Loss=0.243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (inf ---> 0.24320906121953786)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 156/156 [00:13<00:00, 11.25it/s, Epoch=2, LR=3.17e-5, Train_Loss=0.272]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:00<00:00, 29.23it/s, Epoch=2, LR=3.17e-5, Valid_Loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (0.24320906121953786 ---> 0.24242015708264686)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 156/156 [00:13<00:00, 11.21it/s, Epoch=3, LR=2e-6, Train_Loss=0.27]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:00<00:00, 29.40it/s, Epoch=3, LR=2e-6, Valid_Loss=0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (0.24242015708264686 ---> 0.24116663281008494)\n",
      "Model Saved\n",
      "\n",
      "Training complete in 0h 0m 48s\n",
      "Best Loss: 0.2412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, valid_df = train_test_split(train, test_size=0.1, random_state=42)\n",
    "\n",
    "train_loader, valid_loader = prepare_loaders(train_df, valid_df)\n",
    "\n",
    "model = PCL_Model_Arch()\n",
    "model.to(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=500, eta_min=1e-6)\n",
    "\n",
    "model, history = run_training(model, optimizer, scheduler,\n",
    "                              device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                              num_epochs=3)\n",
    "\n",
    "del model, history, train_loader, valid_loader\n",
    "_ = gc.collect()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d292e11c-8df0-44e9-b080-f4cc9f50abf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 156/156 [00:13<00:00, 11.18it/s, Epoch=1, LR=7.81e-5, Train_Loss=0.272]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:00<00:00, 28.86it/s, Epoch=1, LR=7.81e-5, Valid_Loss=0.244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (inf ---> 0.24422530100928794)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 156/156 [00:14<00:00, 11.14it/s, Epoch=2, LR=3.17e-5, Train_Loss=0.273]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:00<00:00, 29.22it/s, Epoch=2, LR=3.17e-5, Valid_Loss=0.244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 156/156 [00:14<00:00, 11.13it/s, Epoch=3, LR=2e-6, Train_Loss=0.271]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:00<00:00, 29.22it/s, Epoch=3, LR=2e-6, Valid_Loss=0.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (0.24422530100928794 ---> 0.23996330379582137)\n",
      "Model Saved\n",
      "\n",
      "Training complete in 0h 0m 47s\n",
      "Best Loss: 0.2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader = prepare_loaders(train_df, valid_df)\n",
    "\n",
    "model = PCL_Model_Arch()\n",
    "model.to(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Define Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=500, eta_min=1e-6)\n",
    "\n",
    "# Train the model\n",
    "model, history = run_training(model, optimizer, scheduler,\n",
    "                              device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                              num_epochs=3)\n",
    "\n",
    "# Clean up\n",
    "del model, history, train_loader, valid_loader\n",
    "_ = gc.collect()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "258aa4a7-3a69-4d1f-8d6c-736badbbf345",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28b2fafd-4ac9-4f4a-a658-eeb0d30c6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = PCLTrainDataset(test, tokenizer=tokenizer, max_length=120)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, \n",
    "                         num_workers=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d85aa36e-8891-4f2f-9333-1713d2dc58fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    PREDS = []\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        ids = data['text_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['text_mask'].to(device, dtype = torch.long)\n",
    "        outputs = model(ids, mask)\n",
    "        PREDS.append(outputs.detach().cpu().numpy()) \n",
    "    \n",
    "    PREDS = np.concatenate(PREDS)\n",
    "    gc.collect()\n",
    "    \n",
    "    return PREDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b2af038-8f29-4808-80b7-4b40021226c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model_paths, dataloader, device):\n",
    "    final_preds = []\n",
    "    for i, path in enumerate(model_paths):\n",
    "        model = PCL_Model_Arch()\n",
    "        model.to(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        \n",
    "        print(f\"Getting predictions for model {i+1}\")\n",
    "        preds = valid_fn(model, dataloader, device)\n",
    "        final_preds.append(preds)\n",
    "    \n",
    "    final_preds = np.array(final_preds)\n",
    "    final_preds = np.mean(final_preds, axis=0)\n",
    "    final_preds= np.argmax(final_preds,axis=1)\n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ea23c48-8c3d-42e3-8dd8-e772cde99aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:01<00:00, 32.74it/s]\n"
     ]
    }
   ],
   "source": [
    "#MODEL_PATH_2=['bert_tweet/Loss-Fold-0.bin','bert_tweet/Loss-Fold-1.bin','bert_tweet/Loss-Fold-2.bin','bert_tweet/Loss-Fold-3.bin','bert_tweet/Loss-Fold-4.bin']\n",
    "MODEL_PATH_2=['bert_tweet/Loss-Fold-0.bin']\n",
    "preds = inference(MODEL_PATH_2, valid_loader, torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd75680e-f31c-420c-ab8b-71c8b2eb7dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score,f1_score,accuracy_score,recall_score,precision_score,classification_report\n",
    "def print_statistics(y, y_pred):\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision =precision_score(y, y_pred, average='weighted')\n",
    "    recall = recall_score(y, y_pred, average='weighted')\n",
    "    f_score = f1_score(y, y_pred, average='weighted')\n",
    "    print('Accuracy: %.3f\\nPrecision: %.3f\\nRecall: %.3f\\nF_score: %.3f\\n'\n",
    "          % (accuracy, precision, recall, f_score))\n",
    "    print(classification_report(y, y_pred))\n",
    "    return accuracy, precision, recall, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adcf6130-b220-499e-95c6-6587417193f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.766\n",
      "Precision: 0.587\n",
      "Recall: 0.766\n",
      "F_score: 0.665\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87       531\n",
      "           1       0.00      0.00      0.00       162\n",
      "\n",
      "    accuracy                           0.77       693\n",
      "   macro avg       0.38      0.50      0.43       693\n",
      "weighted avg       0.59      0.77      0.66       693\n",
      "\n",
      "(0.7662337662337663, 0.587114184516782, 0.7662337662337663, 0.664820473644003)\n"
     ]
    }
   ],
   "source": [
    "print(print_statistics(test['sarcastic'],preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906070dc-6b4c-49e4-bff6-ab157d7574d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
